---
title: "Demonstration of SuperLearner"
author: "JAS"
date: ' '
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Demonstration of SuperLearner

This demonstration will walk you through the basics of using SuperLearner. 
This demonstration is based on the SuperLearner vignette posted on the CRAN in R. You can find it here for additional detail: 

https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html

https://rdrr.io/cran/SuperLearner/


***

### Data Description

This demonstration will utilize the 2019 County Health Rankings. The rankings provide data on a number of demographic, social and environmental health characteristics for counties in the United States. We are using these data to try to predict the counties with greater rates of firearm fatalities based on other county-level characteristics.


***

### Load Packages
SuperLearner does not itself contain a number of the algorithms it utilizes in its libraries. It makes calls to the individual packages for those algorithms. Therefore, you need to load all of the R packages that you want SuperLearner to be able to access. In this case, we are loading caret, randomForest, glmnet and e1071 for svms, in addition to Super Learner. Remember, you'll need to install any new packages before you can load them.


```{r packages}


library(caret)
library(randomForest)
library(glmnet)
library(e1071)
library(ggplot2)
library(SuperLearner)


```

### Read-in and clean data


```{r data_prep}

chr<-read.csv("C:\\Users\\js5406\\OneDrive - cumc.columbia.edu\\EPIC Course\\chr.csv")

chr<-chr[,2:68]

var.names<-c("pre_death", "poorhealth", "poorphyshealth_days", "poormenthealth_days", "low_bwt", "ad_smoking", "ad_obesity", "foodenv_index", "phys_inactivity", "exer_access", "excess_drink", "alc_drivdeaths", "sti", "teen_birth", "uninsured", "primcareproviders", "dentists", "menthealthproviders", "prevhosp", "mammo_screen", "flu_vacc", "hsgrad", "somecollege", "unemployed", "child_poverty", "income_ineq", "sing_parent", "social_assoc", "violent_crime", "injury_deaths", "pm_air", "water_viol", "housing_prob", "driving_alone", "long_commute", "life_exp", "age_adj_premortality", "freq_physdistress", "freq_mentdistress", "diabetes", "hiv", "food_insecure", "ltd_access_healthyfood", "mvcrash_deaths", "insuff_sleep", "uninsured_adults", "uninsured_child", "other_pcp", "medhhinc", "freelunch_child", "res_seg_bw", "res_seg_nw", "firearm_fatalities", "homeownership", "hous_cost_burden", "population", "bw18", "gte65", "nonhisp_afam", "AmerInd_AlasNative", "Asian", "OPacIslander", "Hisp", "nonhisp_white", "nonprof_english", "female", "rural")

colnames(chr)<-var.names

#Will idenitify any rows that do not have complete cases (i.e. have missing data) 
miss.rows<-chr[!complete.cases(chr),]
#There aren't any missing data.

#Create  an indicator of having fire-arm fatalities above the median
#SuperLearner requires the response variable to be encoded if it is a classification problem. So change to 0-1 encoding.

chr$firearm.class<-as.numeric(ifelse(chr$firearm_fatalities>median(chr$firearm_fatalities),1,0))

#Remove continuous version of firearm fatalities variable
chr$firearm_fatalities<-NULL

#Check balance in data
table(chr$firearm.class)

```

### Prepare data for use in SuperLearner and partition into training and testing

Note that all of our variables are numeric. If we had factor variables we would need to expand them into indicator variables using model.matrix.

```{r data_part}
set.seed (123)

#Extract outcome variable
firearm.var<-data.frame(chr$firearm.class)

#Remove outcome variable from dataset so only contains features
chr.features<-subset(chr, select=-firearm.class)

#Partition into training and testing

train.indices<-createDataPartition(y=chr$firearm.class, p=0.7,list=FALSE)

train.features<-chr.features[train.indices,]
test.features<-chr.features[-train.indices,]

#Create vector of outcomes for training/testing sets
train.y<-(firearm.var[train.indices,])
test.y<-(firearm.var[-train.indices,])


```

### Examine algorithms available in SuperLearner and fit a single algorithm

```{r single_alg}
set.seed(123)

listWrappers()

#Screening algorithms wrapper can be used for automated feature selection. Often combined in a pipeline with prediction algorithm wrapper.

# Run an individual model with SuperLearner

sl_lasso<-SuperLearner(Y=train.y, X=train.features, family=binomial(), SL.library="SL.glmnet")

names(sl_lasso)

sl_lasso$coef
#Gives the weight of the algorithm in the overall model

sl_lasso$cvRisk
#Gives the error (Risk) produced by the algorithm

sl_lasso

```

### Enter multiple algorithms in library and interpret the comparison

```{r mult_alg}
set.seed(123)

sl_mult<-SuperLearner(Y=train.y, X=train.features, family=binomial(), SL.library=c("SL.glmnet", "SL.svm", "SL.randomForest"))

sl_mult

#Identify which has the optimal performance
sl_mult$cvRisk[which.min(sl_mult$cvRisk)]


```

### Predict on test set and evaluate

Note that the onlySL option removes algorithms with a weight of 0. It reduces computational time.

We will be examining area under the curve as evaluate. Note that you can compute accuracy directly by outputting predictions and comparing to original values as we've done in past examples. In this example, we are calling to the ROCR package to obtain the area under the curve. It is stored in the y.values object. 

```{r predict}
pred.results<-predict(sl_mult, test.features, onlySL=T)

str(pred.results)

#Examine area under the curve as an evaluation metric

pred_and_actual<-ROCR::prediction(pred.results$pred, test.y)
auc<-ROCR::performance(pred_and_actual, measure="auc", x.measure="cutoff")

auc@y.values[[1]]

```

### Cross-Validation of the Ensemble. 

```{r cross_val}
set.seed(123)
sl_cv<-CV.SuperLearner(Y=train.y, X=train.features, V=5, family=binomial(), SL.library=c("SL.glmnet", "SL.svm", "SL.randomForest"))

summary(sl_cv)

#Distribution of best single learner across the V folds
table(simplify2array(sl_cv$whichDiscreteSL))

plot(sl_cv)+theme_bw()

```

### Tune individual algorithms

Example of varying mtry within random forest within SL

```{r tuning}
set.seed(123)
SL.randomForest

#Create sequence of three different values of mtry
mtry_seq<-floor(sqrt(ncol(train.features)) * c(0.5, 1, 2))

learners<-create.Learner("SL.randomForest", tune=list(mtry=mtry_seq))

SL.randomForest_2

sl_cv.2<-CV.SuperLearner(Y=train.y, X=train.features, V=5, family=binomial(), SL.library=c("SL.glmnet", "SL.svm", learners$names, "SL.randomForest"))

summary(sl_cv.2)
plot(sl_cv.2)+theme_bw()
sl_cv.2$coef

```

### Use best model from CV on test set

```{r predict2}
set.seed(123)

sl_rf.final<-SuperLearner(Y=train.y, X=train.features, family=binomial(), SL.library="SL.randomForest_3")

pred.results.2<-predict(sl_rf.final, test.features, onlySL=T)

str(pred.results.2)

#Examine area under the curve as an evaluation metric

pred_and_actual<-ROCR::prediction(pred.results.2$pred, test.y)
auc.2<-ROCR::performance(pred_and_actual, measure="auc", x.measure="cutoff")

auc.2@y.values[[1]]
```

